{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generator - Winnie the Pooh \n",
    "\n",
    "## Part 1: Processsing Texts and Vector Database\n",
    "\n",
    "[1. Imports and environment](#1-imports-and-environment)\n",
    "\n",
    "[2. Inspect text](#2-inspect-text)\n",
    "\n",
    "[3. Load and chunk data](#3-load-and-chunk-data)\n",
    "\n",
    "[4. Chroma database](#4-chroma-database)\n",
    "\n",
    "[5. Execute](#5-execute)\n",
    "\n",
    "[6. Querying database](#6-querying-database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv langchain langchain-community langchain-openai chromadb chromadbx pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil \n",
    "from dotenv import load_dotenv \n",
    "import re \n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # for chunking text \n",
    "import chromadb \n",
    "from chromadb.utils import embedding_functions # ChromaDB embedding functions\n",
    "from chromadbx import UUIDGenerator # for generating UUIDs \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "load_dotenv()\n",
    "#openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = \"../data/raw\"\n",
    "CHROMA_PATH = \"../data/chroma_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 79 \n",
      "\n",
      "Non-alphanumeric characters: \n",
      "{' ': 23567, '-': 682, '\\n': 3612, '_': 582, '.': 1692, '\"': 2417, \"'\": 689, '&': 2, ',': 2187, '?': 342, '(': 27, ')': 27, '!': 252, ';': 55, '*': 38, ':': 43} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_PATH, \"winnie_the_pooh.txt\"), 'r', encoding='utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "char_count = {}\n",
    "\n",
    "# Count the number of times each character appears in the text\n",
    "for char in full_text:\n",
    "    if char in char_count:\n",
    "        char_count[char] += 1\n",
    "    else:\n",
    "        char_count[char] = 1\n",
    "\n",
    "print(f\"Number of unique characters: {len(char_count.keys())} \\n\")\n",
    "\n",
    "# Filter only non-alphanumeric characters\n",
    "filtered_char_count = {char: count for char, count in char_count.items() if not char.isalnum()}\n",
    "print(\"Non-alphanumeric characters: \")\n",
    "print(filtered_char_count, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    _BY A. A. MILNE_\n",
      "                    _JUVENILES_\n",
      "    \"_The best book of ver\n",
      "ses for children_ _ever written._\"--A. \n",
      "    NEWTON in _The Atlantic Monthly\n",
      "_.\n",
      "                    _ESSAYS_\n",
      "                    _MYSTERY STORY_\n",
      " voice, \"What about _Me_?\" \"My dear Pigle\n",
      "saying, \"What about _Us_?\" So perhaps the\n",
      "on't you know what '_ther_' means?\"\n",
      "\"_What_ about a story?\" I s\n",
      "mself. Because he's _that_ sort of Bear.\"\n",
      "(_\"What does 'under th\n",
      "d Christopher Robin._\n",
      "\"_It means he had the \n",
      "under it._\"\n",
      "_\"Winnie-the-Pooh was\n",
      "d Christopher Robin._\n",
      "_\"Now I am,\" said a g\n",
      "                 *        *        *        *        *\n",
      "                 *        *        *        *        *\n",
      "                 *        *        *        *        *\n",
      "BANG!!!???***!!!\n",
      "                 *        *        *        *        *\n",
      "                 *        *        *        *        *\n",
      "                 *        *        *        *        *\n",
      "                 *        *        *        *        *\n"
     ]
    }
   ],
   "source": [
    "# find instances of underscores in the text to see how they are used \n",
    "matches_1 = re.findall(r'.{0,20}\\_.{0,20}', full_text)\n",
    "for match in matches_1[:20]:\n",
    "    print(match)\n",
    "\n",
    "# find instances of asterisks in the text to see how they are used \n",
    "matches_2 = re.findall(r'.{0,30}\\*.{0,30}', full_text)\n",
    "for match in matches_2:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Underscores and asterisks are not typical punctuations, so I want to check how they are being used in the text. Based on the results above, they both appear to be superfluous, and will be premoved in the processing/chunking steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(filepath, filename=\"winnie_the_pooh.txt\"):\n",
    "  \"\"\"\n",
    "  Load TXT documents from the specified directory. \n",
    "  Remove whitespace and split into individual stories by chapter.\n",
    "  Filename defaults to winnie_the_pooh.txt, but can be specified for other files. \n",
    "\n",
    "  Returns:\n",
    "    List of chapters, removing first title pages.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(os.path.join(filepath, filename), 'r', encoding='utf-8') as file:\n",
    "    story = file.read()\n",
    "\n",
    "    # remove whitespace, underscores, and asterisks\n",
    "    story = re.sub(r'\\s+', ' ', story)\n",
    "    story = re.sub(r\"_\", \"\", story)\n",
    "    story = re.sub(r\"\\*\", \"\", story)\n",
    "\n",
    "    #split into individual stories by chapter\n",
    "    stories_list = story.split(\"CHAPTER \")\n",
    "    \n",
    "    return stories_list[1:] # first element is title pages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stories: 10 \n",
      "\n",
      "Length of first story: 11613 \n",
      "\n",
      "Preview of first story: \n",
      "I IN WHICH WE ARE INTRODUCED TO WINNIE-THE-POOH AND SOME BEES, AND THE STORIES BEGIN Here is Edward Bear, coming downstairs now, bump, bump, bump, on  ...\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(DATA_PATH) \n",
    "\n",
    "# Check results are as expected\n",
    "print(f\"Number of stories: {len(documents)} \\n\")\n",
    "print(f\"Length of first story: {len(documents[0])} \\n\")\n",
    "print(\"Preview of first story: \")\n",
    "print(f\"{documents[0][:150]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(text_list):\n",
    "  \"\"\"\n",
    "  Split the text content of the given list of texts into smaller chunks.\n",
    "  Args:\n",
    "    text_list: List of documents/chapters containing text content to be split.\n",
    "  Returns:\n",
    "    list of chunks: List of Document objects representing the split text chunks.\n",
    "    metadata: dataframe containing metadata for each chunk.\n",
    "  \"\"\"\n",
    "  # Initialize text splitter with specified parameters\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, # Size of each chunk in characters\n",
    "    chunk_overlap=50, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "    )\n",
    "\n",
    "  # Split documents into smaller chunks using text splitter\n",
    "  chunks = [text_splitter.split_text(doc) for doc in text_list]\n",
    "\n",
    "  text = []\n",
    "  title = []\n",
    "  author = []\n",
    "  chapter = []\n",
    "  chunk = []\n",
    "\n",
    "  for i, story in enumerate(chunks):\n",
    "      for j, c in enumerate(story):\n",
    "          text.append(c)\n",
    "          title.append(\"Winnie the Pooh\")\n",
    "          author.append(\"A. A. Milne\")\n",
    "          chapter.append(float(i+1))\n",
    "          chunk.append(float(j))\n",
    "\n",
    "  #metadata = {'title': title, 'author': author, 'chapter': chapter, 'chunk': chunk}\n",
    "  metadata_df = pd.DataFrame({'title': title, 'author': author, 'chapter': chapter, 'chunk': chunk})\n",
    "\n",
    "  return text, metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text chunks: 796\n",
      "Length of first text chunk: 200\n",
      "Preview of text chunks: \n",
      "  I IN WHICH WE ARE INTRODUCED TO WINNIE-THE-POOH AND SOME BEES, AND THE STORIES BEGIN Here is Edward  ...\n",
      "  back of his head, behind Christopher Robin. It is, as far as he knows, the only way of coming downst ...\n",
      "  another way, if only he could stop bumping for a moment and think of it. And then he feels that perh ...\n",
      "  at the bottom, and ready to be introduced to you. Winnie-the-Pooh. When I first heard his name, I sa ...\n",
      "  was a boy?\" \"So did I,\" said Christopher Robin. \"Then you can't call him Winnie?\" \"I don't.\" \"But yo ...\n"
     ]
    }
   ],
   "source": [
    "text, metadata = split_text(documents)\n",
    "\n",
    "# Inspect text chunks\n",
    "print(f\"Total number of text chunks: {len(text)}\")\n",
    "print(f\"Length of first text chunk: {len(text[0])}\")\n",
    "print(\"Preview of text chunks: \")\n",
    "for chunk in text[:5]:\n",
    "    print(\" \", chunk[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of metadata entries: 796\n",
      "Preview of metadata: \n",
      "             title       author  chapter  chunk\n",
      "0  Winnie the Pooh  A. A. Milne      1.0    0.0\n",
      "1  Winnie the Pooh  A. A. Milne      1.0    1.0\n",
      "2  Winnie the Pooh  A. A. Milne      1.0    2.0\n",
      "3  Winnie the Pooh  A. A. Milne      1.0    3.0\n",
      "4  Winnie the Pooh  A. A. Milne      1.0    4.0 \n",
      "\n",
      "Chapters: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.] \n",
      "\n",
      "Chunks: 0 - 109 \n",
      "\n",
      "Chapter 1: 78 chunks\n",
      "Chapter 2: 54 chunks\n",
      "Chapter 3: 46 chunks\n",
      "Chapter 4: 52 chunks\n",
      "Chapter 5: 93 chunks\n",
      "Chapter 6: 96 chunks\n",
      "Chapter 7: 104 chunks\n",
      "Chapter 8: 110 chunks\n",
      "Chapter 9: 94 chunks\n",
      "Chapter 10: 69 chunks\n"
     ]
    }
   ],
   "source": [
    "# Inspect metadata\n",
    "print(f\"Total number of metadata entries: {len(metadata)}\")\n",
    "print(\"Preview of metadata: \")\n",
    "print(metadata.head(), \"\\n\")\n",
    "\n",
    "# Chapters\n",
    "print(f\"Chapters: {metadata['chapter'].unique()} \\n\")\n",
    "\n",
    "# Chunks\n",
    "chunk_min = int(metadata['chunk'].min())\n",
    "chunk_max = int(metadata['chunk'].max())\n",
    "print(f\"Chunks: {chunk_min} - {chunk_max} \\n\")\n",
    "\n",
    "for chapter in metadata['chapter'].unique():\n",
    "    chapter_chunks = metadata[metadata['chapter'] == chapter]\n",
    "    print(f\"Chapter {int(chapter)}: {int(chapter_chunks['chunk'].max())+1} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list, metadata_df: pd.DataFrame):\n",
    "  \"\"\"\n",
    "  Save the given list of chunks and metadata to a Chroma database.\n",
    "  Args:\n",
    "    chunks (list): List of split texts representing text chunks to save.\n",
    "    metadata_df (pd.DataFrame): DataFrame containing metadata for each chunk.\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  \n",
    "  # Clear out the existing database directory if it exists\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "\n",
    "  # Create a new Chroma vector database & collection\n",
    "  client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "  collection = client.get_or_create_collection(name=\"winnie_the_pooh\", embedding_function=default_ef)\n",
    "  \n",
    "  collection.add(\n",
    "    documents=chunks,\n",
    "    metadatas=[dict(metadata_df.iloc[i]) for i in range(len(metadata_df))], \n",
    "    ids=UUIDGenerator(len(chunks))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store():\n",
    "  \"\"\"\n",
    "  Function to generate vector database in chroma from documents.\n",
    "  \"\"\"\n",
    "  documents = load_documents(DATA_PATH) # Load documents from a source\n",
    "  chunks, metadata = split_text(documents) # Split documents into manageable chunks\n",
    "  save_to_chroma(chunks, metadata) # Save the processed data to a data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "# Generate the data store\n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Querying database\n",
    "\n",
    "Checking the database is populated with expected vectors and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'distances', 'metadatas', 'embeddings', 'documents', 'uris', 'data', 'included'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = client.get_collection(name=\"winnie_the_pooh\", embedding_function=default_ef)\n",
    "\n",
    "results = collection.query(query_texts='honey', n_results=3)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Distances: [[1.08407461643219, 1.0937856435775757, 1.1015079021453857]] \n",
      "\n",
      "Chapter: 1.0, Chunk: 17.0\n",
      "then he got up, and said: \"And the only reason for making honey is so as I can eat it.\" So he began to climb the tree. He climbed and he climbed and he climbed, and as he climbed he sang a little  \n",
      "\n",
      "Chapter: 5.0, Chunk: 29.0\n",
      "so as not to hurt myself, and I would get to the Jar of Honey, and I should lick round the edges first of all, pretending that there wasn't any more, you know, and then I should walk away and think  \n",
      "\n",
      "Chapter: 5.0, Chunk: 51.0\n",
      "there. A full jar, full of honey right up to the top, and it had HUNNY written on it, so that I should know it was honey. That's very funny.\" And then he began to wander up and down, wondering where  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cosine Similarity Distances: {results['distances']} \\n\")\n",
    "\n",
    "for i in range(len(results['documents'][0])):\n",
    "    print(f\"Chapter: {results['metadatas'][0][i]['chapter']}, Chunk: {results['metadatas'][0][i]['chunk']}\")\n",
    "    print(results['documents'][0][i], \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['81857d41-f75e-4db3-98ee-9d6bbaf14858',\n",
       "   '8d87e2f0-abc3-455f-8699-677b34fcd7a2',\n",
       "   '4945641e-bee9-45fe-aa2b-0241dc80cd34']],\n",
       " 'distances': [[1.08407461643219, 1.0937856435775757, 1.1015079021453857]],\n",
       " 'metadatas': [[{'author': 'A. A. Milne',\n",
       "    'chapter': 1.0,\n",
       "    'chunk': 17.0,\n",
       "    'title': 'Winnie the Pooh'},\n",
       "   {'author': 'A. A. Milne',\n",
       "    'chapter': 5.0,\n",
       "    'chunk': 29.0,\n",
       "    'title': 'Winnie the Pooh'},\n",
       "   {'author': 'A. A. Milne',\n",
       "    'chapter': 5.0,\n",
       "    'chunk': 51.0,\n",
       "    'title': 'Winnie the Pooh'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['then he got up, and said: \"And the only reason for making honey is so as I can eat it.\" So he began to climb the tree. He climbed and he climbed and he climbed, and as he climbed he sang a little',\n",
       "   \"so as not to hurt myself, and I would get to the Jar of Honey, and I should lick round the edges first of all, pretending that there wasn't any more, you know, and then I should walk away and think\",\n",
       "   'there. A full jar, full of honey right up to the top, and it had HUNNY written on it, so that I should know it was honey. That\\'s very funny.\" And then he began to wander up and down, wondering where']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
